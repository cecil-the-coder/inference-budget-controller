apiVersion: inference.eh-ops.io/v1alpha1
kind: InferenceBackend
metadata:
  name: llamacpp-cpu
  labels:
    app.kubernetes.io/name: inference-budget-controller
    app.kubernetes.io/part-of: inference-budget-controller
spec:
  image:
    repository: ghcr.io/ggml-org/llama.cpp
    tag: server
  port: 8080
  args:
    - llama-server
    - -m
    - $(HF_SOURCE)
    - --host
    - 0.0.0.0
    - --port
    - "8080"
    - --metrics
    - -c
    - "8192"
  readinessPath: /health
